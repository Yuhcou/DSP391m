# Elevator RL Implementation — Detailed Design & Rationale

This document explains the implementation I provided earlier (a simplified, runnable PyTorch implementation inspired by *Novel RL Approach for Efficient Elevator Group Control Systems*). It focuses on **what** was implemented, **which methods** were used, and **why** those choices were made. Use this as developer-facing documentation to understand, run, or extend the code.

---

# 1. High-level Overview

This project implements a simulation and an RL agent to solve the elevator dispatching problem in a multi-elevator building. Key aspects:

- **Environment**: discrete-event simulator with configurable numbers of elevators and floors. New passenger hall calls are generated by a simplified three-peak arrival model.
- **Action space**: combinatorial assignment that allows the agent to select 1, 2, or 3 elevators to respond to a new hall call. For 6 elevators this yields 41 possible actions (C(6,1)+C(6,2)+C(6,3)).
- **Agent**: a Dueling Double Deep Q-Network (DDQN) implemented in PyTorch that outputs Q-values for the 41 actions.
- **Reward**: negative ETD (Estimated Time to Destination) averaged across assigned elevators — the agent minimizes ETD.

The implementation trades simulation complexity for clarity and repeatability while mimicking the three main innovations from the paper: compact action encoding (combinatorial), event-driven decision timing (only on new hall calls), and ETD-based performance objective.

---

# 2. Design Goals & Motivations

**Goals**

1. Create a *clear, runnable prototype* that captures the essential algorithmic choices in the paper.
2. Keep the environment and agent modular so components can be swapped or refined.
3. Use standard, widely-available libraries (NumPy, PyTorch) for portability.

**Motivations behind key choices**

- **Decision timing (event-driven)**: limit decisions to the moment when a *new hall call* appears. This avoids the explosion of action frequency (micromanaging every floor event) while keeping decisions local and tractable.

- **Combinatorial action encoding (choose 1–3 elevators)**: captures multi-elevator responsiveness while keeping the action dimension small enough (41 outputs) for a value-based DQN. Choosing up to 3 elevators balances flexibility and tractability.

- **Dueling Double DQN**: stabilizes learning by:
  - Using *dueling* architecture (separate value and advantage streams) to improve value estimation across large state spaces.
  - Using *double Q-learning* style target calculation to reduce overestimation bias in Q-values.

- **ETD reward**: aligns the learning objective with passenger-centric metrics — not only waiting time, but full trip cost (wait + ride). ETD is more informative than waiting time alone for dispatch quality.

---

# 3. Environment Implementation (ElevatorEnv)

## 3.1. Core state and time model

- **State vector**: a simple flat vector containing:
  - Elevator positions (integer floor indices) for each elevator.
  - Current pending hall call (origin, destination) encoded as two integer values.

- **Time**: the prototype uses discrete time steps (integer increments) with one generated call per step (for simplicity). The full paper’s infra-steps and Poisson arrivals can be layered on top of this prototype.

## 3.2. Three-peak arrival model (simplified)

- The example provided a compact function `_three_peak_rate(t)` to express relative intensity across the simulated day. In the prototype we used a per-step generator that simply creates a new call each step and used three-peak intensity as a template: in a longer-run implementation you should sample Poisson arrivals with a time-varying rate λ(t) matching the three peaks.

## 3.3. Action handling (combinatorial assignments)

- **Action mapping**: before runtime the environment precomputes the combination list `actions = [comb(6,1), comb(6,2), comb(6,3)]` producing 41 action tuples (each tuple is a tuple of elevator indices).

- **Agent step**: when the agent picks an action index `a`, the environment:
  1. Looks up which elevator(s) to assign.
  2. Computes per-assigned-elevator ETD as `distance_to_origin + travel_distance` (a simplified ETD formula).
  3. Updates the selected elevator positions to the passenger destination (instant movement abstraction used for simplicity). In a more realistic simulator you should evolve position over time including door times and stops.

- **Why this design**: By restricting assignment to 1–3 elevators and only when a new hall call appears, this design reduces action dimensionality while still allowing multi-elevator redundancy/backup to be learned.

## 3.4. Reward

- Reward for a step is `-mean(etd_list)` (negative mean ETD across selected elevators). Lower ETD produces larger (less negative) reward.

- **Extensions**: In production-like simulations use a more nuanced ETD calculation that includes: boarding time, door open/close, elevator movement time per floor, stopping penalties, and grouping effects.

---

# 4. Agent Implementation (Dueling Double DQN)

## 4.1. Network architecture

- **Shared layers**: two dense layers (128 units each, ReLU) to build a shared representation of the state.
- **Value stream**: fully connected head to scalar value V(s).
- **Advantage stream**: fully connected head to action-dim advantages A(s,a).
- **Combine**: Q(s,a) = V(s) + A(s,a) - mean_a A(s,a).

**Why dueling?**
- In many states the choice of action matters little; a value stream helps the network learn state-value independently from the advantage of particular assignments.

## 4.2. Double Q target logic (stabilization)

- **Target generation** uses the *current online network* to pick argmax `a' = argmax_a Q_online(s', a)` then *uses the target network* to evaluate `Q_target(s', a')`. This reduces overestimation bias compared to vanilla DQN.

- The target: `y = r + gamma * Q_target(s', argmax(Q_online(s', .)))`.

## 4.3. Replay buffer

- Experience replay is implemented as a simple FIFO `deque` storing `(s, a, r, s')` tuples.
- Sampling randomly breaks correlations between consecutive transitions.

## 4.4. Exploration & target updates

- **Epsilon-greedy** exploration schedule: start with `eps=1.0` then exponentially decay to a floor (e.g., 0.05).
- **Target network soft update**: a polyak-style update was used in the prototype (`target = tau*online + (1-tau)*target`) for stability.

---

# 5. Training Loop & Hyperparameters

## 5.1. Training loop (prototype)

- For each episode (simulated day), reset the env and run for `T` steps.
- At each step: agent selects `a` from 41 options, env returns `s', r`, and the transition is stored in replay buffer.
- Periodically sample a minibatch and perform a gradient step on the DQN loss.
- Soft update target network after each episode or each step.

## 5.2. Example hyperparameters (used in prototype)

- `state_dim = n_elevators + 2` (positions + call origin/dest)
- `action_dim = 41`
- `hidden = [128, 128]`
- `batch_size = 64`
- `buffer_capacity = 10000`
- `gamma = 0.99`
- `lr = 1e-3`
- `eps_start = 1.0`, `eps_end = 0.05`, decay `0.995` per episode
- `target_tau = 0.995` (soft update)

---

# 6. Evaluation & Metrics

- **Primary metric**: Mean ETD over evaluation episodes (lower is better).
- **Secondary metrics**: mean waiting time, throughput (# people served per episode), ETD distribution (median, 95th percentile), energy proxy (movement count).
- **Baseline comparison**: implement a naive rule-based dispatcher (nearest car, or a simple queue-based assignment) and compare mean ETD, CDF of wait times, and variance.

---

# 7. Limitations of the Prototype

- **Simplified motion model**: elevators teleport to destination after assignment. Real systems must simulate per-floor motion, door times, and intermediate stops.
- **Simplified arrival process**: calling `generate_call()` every step is a simplification. Production use should sample Poisson arrivals with a time-varying λ(t).
- **State encoding**: the prototype state is very compact; richer states should include direction, car call queues, passenger counts, and waiting times per call.
- **Single-call-per-step decision**: the agent only handles one new call each step. In dense traffic multiple simultaneous arrivals might need batching or prioritized handling.
- **No infra-steps yet**: prototype does not yet implement infra-step granularity. Adding infra-steps requires changes to the environment reward/dynamics and discounting handling in the agent.

---

# 8. Extensions & Next Steps

If you want to move this prototype closer to the paper or to production-grade experiments, consider:

1. **Accurate motion & door model**: simulate time per floor, door open/close times, per-stop delays.
2. **Infra-steps implementation**: add fine-grained substeps to model continuous arrivals and make macro-decisions less frequent but aware of infra-step transitions.
3. **Richer state:** include direction, car-call lists, per-call waiting durations, elevator load/capacity.
4. **Prioritized replay**: accelerate learning for rare but important transitions using prioritized experience replay.
5. **Multi-objective reward**: combine ETD, energy consumption, and fairness; optionally use multi-objective RL.
6. **Evaluation on realistic traces**: use real building logs or more realistic synthetic traces for credible benchmarking.

---

# 9. How to run & dependencies

**Minimal dependencies**:
```bash
pip install numpy torch
```

**Run**: copy the prototype code into a `train.py` and run `python train.py`.

**Visualization**: add `matplotlib` to plot episode rewards or ETD trends:
```bash
pip install matplotlib
```

---

# 10. Appendix: Key Code References (where in the prototype they live)

- `ElevatorEnv` — environment and action combo generation (`_generate_actions`) (env class in the prototype).
- `DuelingDQN` — network architecture (two streams).
- `ReplayBuffer` — simple replay buffer.
- `Agent` — policy, target updates, and learning step.
- `training loop` — example loop with epsilon decay and soft target updates.

---

If you want, I can now:
- Produce an expanded prototype that implements infra-steps and a realistic per-floor motion simulator, or
- Convert this document to a downloadable README or a Jupyter notebook with runnable cells (including visualizations), or
- Generate unit tests for the simulator and agent to guarantee deterministic behavior for specific seeds.

Tell me which you prefer and I will continue.

